{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61400f50-9807-4339-9869-357184d97609",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Q1: What is Elastic Net Regression and how does it differ from other regression techniques?\n",
    "Elastic Net Regression is a linear regression model that combines both Lasso (L1) and Ridge (L2) regularization techniques. \n",
    "The model introduces two penalties: one based on the absolute values of the coefficients (Lasso) and one based on the squared values of the \n",
    "coefficients (Ridge). Elastic Net is particularly useful when dealing with datasets where the number of predictors exceeds the number of \n",
    "observations or when there is multicollinearity among the predictors.\n",
    "\n",
    "Differences from Other Regression Techniques:\n",
    "\n",
    "Ordinary Least Squares (OLS): No regularization, leading to potential overfitting, especially with high-dimensional data.\n",
    "Ridge Regression: Only uses L2 regularization, which shrinks coefficients but does not perform feature selection.\n",
    "Lasso Regression: Only uses L1 regularization, which can shrink some coefficients to zero, performing feature selection.\n",
    "Elastic Net: Combines L1 and L2 regularization, providing a balance between feature selection (Lasso) and coefficient shrinkage (Ridge).\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23b4d902-47e3-43cc-84aa-2292bfb42bf6",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Q2: How do you choose the optimal values of the regularization parameters for Elastic Net Regression?\n",
    "Choosing Optimal Values for Regularization Parameters:\n",
    "\n",
    "Lambda (\n",
    "ùúÜ\n",
    "Œª): Controls the overall strength of the regularization. It can be chosen using cross-validation by testing a range of \n",
    "ùúÜ\n",
    "Œª values and selecting the one that minimizes the cross-validation error.\n",
    "Alpha (\n",
    "ùõº\n",
    "Œ±): Controls the mix between Lasso (L1) and Ridge (L2) penalties.\n",
    "ùõº\n",
    "=\n",
    "1\n",
    "Œ±=1: Pure Lasso regression.\n",
    "ùõº\n",
    "=\n",
    "0\n",
    "Œ±=0: Pure Ridge regression.\n",
    "0 < \n",
    "ùõº\n",
    "Œ± < 1: A combination of Lasso and Ridge.\n",
    "ùõº\n",
    "Œ± is typically chosen through cross-validation or grid search.\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c49c21aa-0a7f-4230-b99b-08aaf2aed285",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Q3: What are the advantages and disadvantages of Elastic Net Regression?\n",
    "Advantages:\n",
    "\n",
    "Handles Multicollinearity: By combining L1 and L2 regularization, Elastic Net can effectively handle multicollinearity in the input features.\n",
    "Feature Selection: Elastic Net performs feature selection (like Lasso) but with added stability due to the Ridge component.\n",
    "Flexibility: The ability to adjust the balance between Lasso and Ridge penalties allows Elastic Net to be tailored to various datasets.\n",
    "Disadvantages:\n",
    "\n",
    "Complexity: Elastic Net introduces an additional tuning parameter (\n",
    "ùõº\n",
    "Œ±), which can increase the complexity of the model selection process.\n",
    "Computational Cost: Depending on the dataset size and number of features, Elastic Net can be computationally intensive, especially when\n",
    "cross-validation is used to tune both \n",
    "ùúÜ\n",
    "Œª and \n",
    "ùõº\n",
    "Œ±.\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ad778c1-0598-4a27-a711-a0f379366257",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Q4: What are some common use cases for Elastic Net Regression?\n",
    "Common Use Cases:\n",
    "\n",
    "High-Dimensional Data: Elastic Net is particularly effective when the number of features exceeds the number of observations (e.g., genetic data).\n",
    "Multicollinearity: Elastic Net is used when there is multicollinearity among predictors, as it can handle correlated variables better than Lasso.\n",
    "Sparse Models: When feature selection is desired, but a purely Lasso approach might be too aggressive, Elastic Net can be used to create a\n",
    "sparse model with better predictive accuracy.\n",
    "Generalized Linear Models: Elastic Net can be extended to generalized linear models (e.g., logistic regression) to handle high-dimensional \n",
    "classification tasks.\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf4a7a13-d483-4324-bb72-11e7ba65bdac",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Q5: How do you interpret the coefficients in Elastic Net Regression?\n",
    "Interpreting Coefficients:\n",
    "\n",
    "Magnitude: The magnitude of a coefficient indicates the strength of the relationship between the feature and the dependent variable.\n",
    "Sign: The sign of a coefficient (positive or negative) indicates the direction of the relationship.\n",
    "Zero Coefficients: Coefficients that are exactly zero indicate that the feature has been excluded from the model, due to the Lasso component of\n",
    "Elastic Net.\n",
    "Bias: The coefficients are regularized, meaning they are biased toward zero. However, the Ridge component ensures that highly correlated predictors \n",
    "are less likely to be completely zeroed out.\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d773837-97aa-48a1-8fb9-eb52c829ff6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Q6: How do you handle missing values when using Elastic Net Regression?\n",
    "Handling Missing Values:\n",
    "\n",
    "Imputation: Before applying Elastic Net Regression, missing values should be imputed. Common techniques include:\n",
    "Mean/Median Imputation: Replacing missing values with the mean or median of the feature.\n",
    "K-Nearest Neighbors (KNN) Imputation: Imputing missing values based on the values of the nearest neighbors.\n",
    "Multiple Imputation: Creating multiple imputed datasets and averaging the results.\n",
    "Dropping Missing Values: If the proportion of missing data is small, the rows or columns with missing values can be dropped.\n",
    "Elastic Net does not handle missing values directly, so preprocessing steps to impute or remove missing data are necessary.\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75d025ac-beb7-411b-976b-d8be0d210ae6",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "\n",
    "'''"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
