{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "150e51fe-c8a7-4ddd-a997-6a7f23e7d24e",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Q1. Explain the difference between linear regression and logistic regression models. Provide an example of a scenario where logistic \n",
    "regression would be more appropriate.\n",
    "Linear Regression is used for predicting a continuous dependent variable based on one or more independent variables. The model assumes a\n",
    "linear relationship between the dependent and independent variables. The output of linear regression is a continuous value, and the cost \n",
    "function used is the mean squared error (MSE).\n",
    "\n",
    "Logistic Regression is used for binary classification tasks where the dependent variable is categorical (usually binary). Instead of \n",
    "predicting a continuous value, it predicts the probability of the dependent variable belonging to a particular class (typically 0 or 1).\n",
    "Logistic regression uses the sigmoid function to map predicted values to probabilities, and the output is a probability value between 0 \n",
    "and 1, which is then classified into one of the two categories based on a threshold.\n",
    "\n",
    "Example Scenario:\n",
    "Logistic regression would be more appropriate in a scenario like predicting whether a customer will buy a product (Yes/No) based on \n",
    "features like age, income, and browsing history. The outcome is binary, so logistic regression is the suitable model.\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bce3a63b-f41c-4ac4-a0e7-90b740a5ca49",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Q2. What is the cost function used in logistic regression, and how is it optimized?\n",
    "The cost function used in logistic regression is the Log Loss (also known as the Binary Cross-Entropy Loss). \n",
    "It measures the performance of the classification model whose output is a probability value between 0 and 1. \n",
    "The formula for the cost function is:\n",
    "\n",
    "Cost\n",
    "(\n",
    "ℎ\n",
    "(\n",
    "𝑥\n",
    ")\n",
    ",\n",
    "𝑦\n",
    ")\n",
    "=\n",
    "−\n",
    "1\n",
    "𝑚\n",
    "∑\n",
    "𝑖\n",
    "=\n",
    "1\n",
    "𝑚\n",
    "[\n",
    "𝑦\n",
    "(\n",
    "𝑖\n",
    ")\n",
    "log\n",
    "⁡\n",
    "(\n",
    "ℎ\n",
    "𝜃\n",
    "(\n",
    "𝑥\n",
    "(\n",
    "𝑖\n",
    ")\n",
    ")\n",
    ")\n",
    "+\n",
    "(\n",
    "1\n",
    "−\n",
    "𝑦\n",
    "(\n",
    "𝑖\n",
    ")\n",
    ")\n",
    "log\n",
    "⁡\n",
    "(\n",
    "1\n",
    "−\n",
    "ℎ\n",
    "𝜃\n",
    "(\n",
    "𝑥\n",
    "(\n",
    "𝑖\n",
    ")\n",
    ")\n",
    ")\n",
    "]\n",
    "Cost(h(x),y)=− \n",
    "m\n",
    "1\n",
    "​\n",
    " ∑ \n",
    "i=1\n",
    "m\n",
    "​\n",
    " [y \n",
    "(i)\n",
    " log(h \n",
    "θ\n",
    "​\n",
    " (x \n",
    "(i)\n",
    " ))+(1−y \n",
    "(i)\n",
    " )log(1−h \n",
    "θ\n",
    "​\n",
    " (x \n",
    "(i)\n",
    " ))]\n",
    "\n",
    "where:\n",
    "\n",
    "𝑦\n",
    "(\n",
    "𝑖\n",
    ")\n",
    "y \n",
    "(i)\n",
    "  is the actual label.\n",
    "ℎ\n",
    "𝜃\n",
    "(\n",
    "𝑥\n",
    "(\n",
    "𝑖\n",
    ")\n",
    ")\n",
    "h \n",
    "θ\n",
    "​\n",
    " (x \n",
    "(i)\n",
    " ) is the predicted probability from the logistic regression model.\n",
    "𝑚\n",
    "m is the number of training examples.\n",
    "This cost function is optimized using Gradient Descent (or one of its variants like Stochastic Gradient Descent, Mini-batch \n",
    "Gradient Descent). The goal is to minimize the cost function by updating the model's parameters iteratively.\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68e139db-f946-4e05-ab3f-ec85791f1e99",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Q3. Explain the concept of regularization in logistic regression and how it helps prevent overfitting.\n",
    "Regularization is a technique used to prevent overfitting by adding a penalty term to the cost function, \n",
    "which discourages the model from fitting too closely to the training data. This penalty term constrains the magnitude of the coefficients\n",
    "(weights) in the model.\n",
    "\n",
    "In logistic regression, two common types of regularization are:\n",
    "\n",
    "L2 Regularization (Ridge Regression): Adds a penalty equal to the square of the magnitude of coefficients ( \n",
    "𝜆\n",
    "∑\n",
    "𝑗\n",
    "=\n",
    "1\n",
    "𝑛\n",
    "𝜃\n",
    "𝑗\n",
    "2\n",
    "λ∑ \n",
    "j=1\n",
    "n\n",
    "​\n",
    " θ \n",
    "j\n",
    "2\n",
    "​\n",
    "  ). This helps in shrinking the coefficients, thereby reducing the model's complexity.\n",
    "\n",
    "L1 Regularization (Lasso Regression): Adds a penalty equal to the absolute value of the magnitude of coefficients ( \n",
    "𝜆\n",
    "∑\n",
    "𝑗\n",
    "=\n",
    "1\n",
    "𝑛\n",
    "∣\n",
    "𝜃\n",
    "𝑗\n",
    "∣\n",
    "λ∑ \n",
    "j=1\n",
    "n\n",
    "​\n",
    " ∣θ \n",
    "j\n",
    "​\n",
    " ∣ ). This can lead to sparse models where some coefficients are exactly zero, effectively performing feature selection.\n",
    "\n",
    "By including these regularization terms, the model is less likely to overfit the training data, improving its generalization to new, \n",
    "unseen data.\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb86f6b8-811e-4947-9600-280b18a74957",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Q4. What is the ROC curve, and how is it used to evaluate the performance of the logistic regression model?\n",
    "The ROC (Receiver Operating Characteristic) Curve is a graphical representation of the performance of a binary classification model. \n",
    "It plots the True Positive Rate (TPR) against the False Positive Rate (FPR) at various threshold settings.\n",
    "\n",
    "True Positive Rate (TPR): Also known as sensitivity or recall, it is the ratio of correctly predicted positive observations to the \n",
    "actual positives.\n",
    "False Positive Rate (FPR): It is the ratio of incorrectly predicted positive observations to the actual negatives.\n",
    "'''"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
