{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41266717-95ca-42cd-8bc7-b089fc3e30f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Q1. What is the Mathematical Formula for a Linear SVM?\n",
    "A Linear Support Vector Machine (SVM) is a classification model that finds the best hyperplane separating two classes in a feature space. The mathematical form of the decision boundary (hyperplane) in a linear SVM is:\n",
    "\n",
    "𝑓\n",
    "(\n",
    "𝑥\n",
    ")\n",
    "=\n",
    "𝑤\n",
    "⊤\n",
    "𝑥\n",
    "+\n",
    "𝑏\n",
    "f(x)=w \n",
    "⊤\n",
    " x+b\n",
    "Where:\n",
    "\n",
    "𝑥\n",
    "x is the input feature vector.\n",
    "𝑤\n",
    "w is the weight vector.\n",
    "𝑏\n",
    "b is the bias term (intercept).\n",
    "A point \n",
    "𝑥\n",
    "x is classified as:\n",
    "\n",
    "Positive class if \n",
    "𝑓\n",
    "(\n",
    "𝑥\n",
    ")\n",
    ">\n",
    "0\n",
    "f(x)>0\n",
    "Negative class if \n",
    "𝑓\n",
    "(\n",
    "𝑥\n",
    ")\n",
    "<\n",
    "0\n",
    "f(x)<0\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9471132-a48d-413a-be79-8fed60579c9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Q2. What is the Objective Function of a Linear SVM?\n",
    "The objective of a linear SVM is to find the hyperplane that maximizes the margin, i.e., the distance between the hyperplane and the nearest data points from each class (support vectors). The objective function is:\n",
    "\n",
    "min\n",
    "⁡\n",
    "𝑤\n",
    ",\n",
    "𝑏\n",
    "1\n",
    "2\n",
    "∥\n",
    "𝑤\n",
    "∥\n",
    "2\n",
    "w,b\n",
    "min\n",
    "​\n",
    "  \n",
    "2\n",
    "1\n",
    "​\n",
    " ∥w∥ \n",
    "2\n",
    " \n",
    "Subject to the constraints for each training sample \n",
    "(\n",
    "𝑥\n",
    "𝑖\n",
    ",\n",
    "𝑦\n",
    "𝑖\n",
    ")\n",
    "(x \n",
    "i\n",
    "​\n",
    " ,y \n",
    "i\n",
    "​\n",
    " ):\n",
    "\n",
    "𝑦\n",
    "𝑖\n",
    "(\n",
    "𝑤\n",
    "⊤\n",
    "𝑥\n",
    "𝑖\n",
    "+\n",
    "𝑏\n",
    ")\n",
    "≥\n",
    "1\n",
    "for all \n",
    "𝑖\n",
    "y \n",
    "i\n",
    "​\n",
    " (w \n",
    "⊤\n",
    " x \n",
    "i\n",
    "​\n",
    " +b)≥1for all i\n",
    "Where:\n",
    "\n",
    "∥\n",
    "𝑤\n",
    "∥\n",
    "∥w∥ is the norm of the weight vector (measuring its magnitude).\n",
    "𝑦\n",
    "𝑖\n",
    "y \n",
    "i\n",
    "​\n",
    "  is the class label for the \n",
    "𝑖\n",
    "ith training sample, with \n",
    "𝑦\n",
    "𝑖\n",
    "∈\n",
    "{\n",
    "−\n",
    "1\n",
    ",\n",
    "1\n",
    "}\n",
    "y \n",
    "i\n",
    "​\n",
    " ∈{−1,1}.\n",
    "The constraints ensure that each data point is correctly classified with a margin of at least 1.\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f3e607b-54e1-49fe-9faf-47a801971261",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Q3. What is the Kernel Trick in SVM?\n",
    "The Kernel Trick is a technique used in SVMs to transform data into a higher-dimensional space where a linear separation is possible. This allows SVMs to create non-linear decision boundaries in the original feature space.\n",
    "\n",
    "The kernel function \n",
    "𝐾\n",
    "(\n",
    "𝑥\n",
    "𝑖\n",
    ",\n",
    "𝑥\n",
    "𝑗\n",
    ")\n",
    "K(x \n",
    "i\n",
    "​\n",
    " ,x \n",
    "j\n",
    "​\n",
    " ) implicitly computes the dot product in this higher-dimensional space without explicitly transforming the data, making the computation efficient.\n",
    "\n",
    "Common kernel functions include:\n",
    "\n",
    "Linear Kernel: \n",
    "𝐾\n",
    "(\n",
    "𝑥\n",
    "𝑖\n",
    ",\n",
    "𝑥\n",
    "𝑗\n",
    ")\n",
    "=\n",
    "𝑥\n",
    "𝑖\n",
    "⊤\n",
    "𝑥\n",
    "𝑗\n",
    "K(x \n",
    "i\n",
    "​\n",
    " ,x \n",
    "j\n",
    "​\n",
    " )=x \n",
    "i\n",
    "⊤\n",
    "​\n",
    " x \n",
    "j\n",
    "​\n",
    " \n",
    "Polynomial Kernel: \n",
    "𝐾\n",
    "(\n",
    "𝑥\n",
    "𝑖\n",
    ",\n",
    "𝑥\n",
    "𝑗\n",
    ")\n",
    "=\n",
    "(\n",
    "𝑥\n",
    "𝑖\n",
    "⊤\n",
    "𝑥\n",
    "𝑗\n",
    "+\n",
    "𝑐\n",
    ")\n",
    "𝑑\n",
    "K(x \n",
    "i\n",
    "​\n",
    " ,x \n",
    "j\n",
    "​\n",
    " )=(x \n",
    "i\n",
    "⊤\n",
    "​\n",
    " x \n",
    "j\n",
    "​\n",
    " +c) \n",
    "d\n",
    " \n",
    "Radial Basis Function (RBF) Kernel: \n",
    "𝐾\n",
    "(\n",
    "𝑥\n",
    "𝑖\n",
    ",\n",
    "𝑥\n",
    "𝑗\n",
    ")\n",
    "=\n",
    "exp\n",
    "⁡\n",
    "(\n",
    "−\n",
    "𝛾\n",
    "∥\n",
    "𝑥\n",
    "𝑖\n",
    "−\n",
    "𝑥\n",
    "𝑗\n",
    "∥\n",
    "2\n",
    ")\n",
    "K(x \n",
    "i\n",
    "​\n",
    " ,x \n",
    "j\n",
    "​\n",
    " )=exp(−γ∥x \n",
    "i\n",
    "​\n",
    " −x \n",
    "j\n",
    "​\n",
    " ∥ \n",
    "2\n",
    " )\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d399a34-d1dd-4a71-a13f-e1353042927d",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Q4. What is the Role of Support Vectors in SVM? Explain with Example.\n",
    "Support Vectors are the data points closest to the decision boundary (hyperplane) in an SVM. These points are critical because they define the margin and thus determine the optimal hyperplane.\n",
    "\n",
    "Role of Support Vectors:\n",
    "\n",
    "They are the only points that influence the position and orientation of the decision boundary.\n",
    "The SVM algorithm tries to maximize the margin around these support vectors.\n",
    "If these points were removed or changed slightly, the decision boundary would be affected.\n",
    "Example:\n",
    "Consider a simple 2D binary classification problem. Suppose the two classes are separable by a straight line. The support vectors are the data points from each class that are nearest to the decision boundary. These points \"support\" the hyperplane, and the margin is the distance from the hyperplane to these points.\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fced06b-6ffd-4dba-ba23-2fda2a3adb01",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Q5. Illustrate with Examples and Graphs of Hyperplane, Marginal Plane, Soft Margin, and Hard Margin in SVM\n",
    "Hyperplane:\n",
    "\n",
    "A hyperplane is the decision boundary that separates the classes.\n",
    "In 2D, it's a line; in 3D, it's a plane; in higher dimensions, it's a hyperplane.\n",
    "Marginal Plane:\n",
    "\n",
    "The marginal planes are the parallel planes to the hyperplane that pass through the support vectors.\n",
    "The distance between these marginal planes is called the margin.\n",
    "Hard Margin:\n",
    "\n",
    "A hard margin SVM assumes that the data is perfectly linearly separable and tries to maximize the margin without any errors (no points inside the margin).\n",
    "Graph: Points lie outside the margin, and the margin is maximized.\n",
    "'''\n",
    "from sklearn import datasets\n",
    "from sklearn.svm import SVC\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Load a simple 2D dataset\n",
    "X, y = datasets.make_blobs(n_samples=100, centers=2, random_state=6)\n",
    "\n",
    "# Fit a linear SVM model with hard margin (C=1e10)\n",
    "svc = SVC(kernel='linear', C=1e10)\n",
    "svc.fit(X, y)\n",
    "\n",
    "# Plot the decision boundary and margins\n",
    "plt.scatter(X[:, 0], X[:, 1], c=y, cmap='coolwarm')\n",
    "\n",
    "# Plot the decision boundary\n",
    "ax = plt.gca()\n",
    "xlim = ax.get_xlim()\n",
    "ylim = ax.get_ylim()\n",
    "\n",
    "xx = np.linspace(xlim[0], xlim[1], 30)\n",
    "yy = np.linspace(ylim[0], ylim[1], 30)\n",
    "YY, XX = np.meshgrid(yy, xx)\n",
    "xy = np.vstack([XX.ravel(), YY.ravel()]).T\n",
    "Z = svc.decision_function(xy).reshape(XX.shape)\n",
    "\n",
    "ax.contour(XX, YY, Z, colors='k', levels=[-1, 0, 1], alpha=0.5,\n",
    "           linestyles=['--', '-', '--'])\n",
    "\n",
    "# Highlight support vectors\n",
    "ax.scatter(svc.support_vectors_[:, 0], svc.support_vectors_[:, 1], s=100,\n",
    "           linewidth=1, facecolors='none', edgecolors='k')\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffb67cf3-0dbd-4404-94c8-96cfbfb71ce1",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"Q6. SVM Implementation through Iris Dataset\"\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
