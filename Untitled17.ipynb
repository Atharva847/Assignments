{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39dab6b6-3b87-4194-9d31-6a70df9c167a",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Q1: What is Ridge Regression, and how does it differ from ordinary least squares regression?\n",
    "Ridge Regression is a type of linear regression that includes a regularization term to prevent overfitting by shrinking the magnitude of the model's\n",
    "coefficients. It modifies the ordinary least squares (OLS) \n",
    "regression by adding a penalty term to the cost function based on the sum of the squared coefficients.\n",
    "\n",
    "Ridge Regression Formula:\n",
    "MinimizeÂ \n",
    "âˆ‘\n",
    "ğ‘–\n",
    "=\n",
    "1\n",
    "ğ‘›\n",
    "(\n",
    "ğ‘¦\n",
    "ğ‘–\n",
    "âˆ’\n",
    "âˆ‘\n",
    "ğ‘—\n",
    "=\n",
    "1\n",
    "ğ‘\n",
    "ğ›½\n",
    "ğ‘—\n",
    "ğ‘‹\n",
    "ğ‘–\n",
    "ğ‘—\n",
    ")\n",
    "2\n",
    "+\n",
    "ğœ†\n",
    "âˆ‘\n",
    "ğ‘—\n",
    "=\n",
    "1\n",
    "ğ‘\n",
    "ğ›½\n",
    "ğ‘—\n",
    "2\n",
    "MinimizeÂ âˆ‘ \n",
    "i=1\n",
    "n\n",
    "â€‹\n",
    " (y \n",
    "i\n",
    "â€‹\n",
    " âˆ’âˆ‘ \n",
    "j=1\n",
    "p\n",
    "â€‹\n",
    " Î² \n",
    "j\n",
    "â€‹\n",
    " X \n",
    "ij\n",
    "â€‹\n",
    " ) \n",
    "2\n",
    " +Î»âˆ‘ \n",
    "j=1\n",
    "p\n",
    "â€‹\n",
    " Î² \n",
    "j\n",
    "2\n",
    "â€‹\n",
    " \n",
    "where:\n",
    "\n",
    "ğœ†\n",
    "Î» is the regularization parameter that controls the strength of the penalty.\n",
    "ğ›½\n",
    "ğ‘—\n",
    "Î² \n",
    "j\n",
    "â€‹\n",
    "  are the coefficients of the independent variables.\n",
    "Differences from Ordinary Least Squares (OLS) Regression:\n",
    "\n",
    "Penalty Term: Ridge regression adds a penalty (\n",
    "ğœ†\n",
    "âˆ‘\n",
    "ğ‘—\n",
    "=\n",
    "1\n",
    "ğ‘\n",
    "ğ›½\n",
    "ğ‘—\n",
    "2\n",
    "Î»âˆ‘ \n",
    "j=1\n",
    "p\n",
    "â€‹\n",
    " Î² \n",
    "j\n",
    "2\n",
    "â€‹\n",
    " ) to the OLS cost function, which discourages large coefficients and helps to reduce overfitting.\n",
    "Coefficient Shrinkage: The coefficients in Ridge regression are typically smaller than in OLS, as the regularization term forces them to be closer to zero.\n",
    "Bias-Variance Tradeoff: While OLS aims to minimize the error between the observed and predicted values, Ridge regression introduces bias to reduce variance, leading to a model that may perform better on unseen data.\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdda923b-a41c-4e73-9de5-6b8cca835b26",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Q2: What are the assumptions of Ridge Regression?\n",
    "Assumptions of Ridge Regression:\n",
    "\n",
    "Linearity: The relationship between the independent and dependent variables is linear.\n",
    "Independence: Observations are independent of each other.\n",
    "Homoscedasticity: The variance of errors is constant across all levels of the independent variables.\n",
    "Normality of Errors: The errors (residuals) are normally distributed.\n",
    "Multicollinearity: Unlike OLS, Ridge regression can handle multicollinearity, but the presence of multicollinearity does influence the regularization \n",
    "effect.\n",
    "\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83178aac-29a1-4355-95e3-e81ca73f24a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Q3: How do you select the value of the tuning parameter (lambda) in Ridge Regression?\n",
    "Selecting the Value of Lambda (\n",
    "ğœ†\n",
    "Î»):\n",
    "\n",
    "Cross-Validation: The most common method is to use cross-validation to select \n",
    "ğœ†\n",
    "Î». By dividing the data into training and validation sets, you can test different \n",
    "ğœ†\n",
    "Î» values and choose the one that minimizes the cross-validation error.\n",
    "Grid Search: Perform a grid search over a range of \n",
    "ğœ†\n",
    "Î» values to find the optimal one.\n",
    "Regularization Path: Some algorithms, like the LARS (Least Angle Regression) algorithm, can be used to explore the regularization path, \n",
    "which shows how coefficients change with different \n",
    "ğœ†\n",
    "Î» values.\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd5b1708-84b1-4281-af0c-08596935b7fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Q4: Can Ridge Regression be used for feature selection? If yes, how?\n",
    "Ridge Regression and Feature Selection:\n",
    "\n",
    "Feature Selection: Ridge regression is not typically used for feature selection because it does not set any coefficients exactly to zero.\n",
    "Instead, it shrinks the coefficients of less important features closer to zero but not to zero,\n",
    "unlike Lasso regression.\n",
    "Interpretation: While Ridge does not perform feature selection, it can still indicate the relative importance of features by comparing the \n",
    "magnitude of the coefficients after regularization. Larger coefficients imply more influential features.\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6803c8fa-3448-4d76-9497-390bb6afd2fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Q5: How does the Ridge Regression model perform in the presence of multicollinearity?\n",
    "Ridge Regression and Multicollinearity:\n",
    "\n",
    "Multicollinearity: Ridge regression is particularly effective in the presence of multicollinearity (when independent variables are highly correlated).\n",
    "Performance: In the presence of multicollinearity, OLS estimates can become unstable and have high variance. Ridge regression reduces the variance by \n",
    "shrinking coefficients, leading to more reliable and interpretable estimates.\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cb757a8-97e2-4f81-af87-b9544b2d3b5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Q6: Can Ridge Regression handle both categorical and continuous independent variables?\n",
    "Handling Categorical and Continuous Variables:\n",
    "\n",
    "Continuous Variables: Ridge regression directly handles continuous independent variables.\n",
    "Categorical Variables: Categorical variables can be included in Ridge regression after they are encoded numerically (e.g., one-hot encoding). \n",
    "The regularization will then apply to the coefficients associated with these encoded variables.\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43ecec71-abcd-4de0-94bf-b660c9e524e0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
