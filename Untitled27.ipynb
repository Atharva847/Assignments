{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "604ba534-710e-4565-a78e-52521f40992a",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "\n",
    "Q1. What is the Relationship Between Polynomial Functions and Kernel Functions in Machine Learning Algorithms?\n",
    "In machine learning, kernel functions are used to map data into a higher-dimensional space, where it may become linearly separable. A polynomial kernel is a specific type of kernel function that applies a polynomial transformation to the input features. The polynomial kernel function is defined as:\n",
    "\n",
    "𝐾\n",
    "(\n",
    "𝑥\n",
    "𝑖\n",
    ",\n",
    "𝑥\n",
    "𝑗\n",
    ")\n",
    "=\n",
    "(\n",
    "𝑥\n",
    "𝑖\n",
    "⊤\n",
    "𝑥\n",
    "𝑗\n",
    "+\n",
    "𝑐\n",
    ")\n",
    "𝑑\n",
    "K(x \n",
    "i\n",
    "​\n",
    " ,x \n",
    "j\n",
    "​\n",
    " )=(x \n",
    "i\n",
    "⊤\n",
    "​\n",
    " x \n",
    "j\n",
    "​\n",
    " +c) \n",
    "d\n",
    " \n",
    "Where:\n",
    "\n",
    "𝑥\n",
    "𝑖\n",
    "x \n",
    "i\n",
    "​\n",
    "  and \n",
    "𝑥\n",
    "𝑗\n",
    "x \n",
    "j\n",
    "​\n",
    "  are input vectors.\n",
    "𝑐\n",
    "c is a constant that trades off the influence of higher-order versus lower-order terms.\n",
    "𝑑\n",
    "d is the degree of the polynomial.\n",
    "Relationship:\n",
    "\n",
    "The polynomial kernel allows an SVM or other kernel-based algorithms to fit non-linear decision boundaries by implicitly mapping the data into a higher-dimensional space where a linear classifier can be applied.\n",
    "For example, a quadratic polynomial kernel can capture quadratic relationships in the data without explicitly computing the polynomial features, thanks to the kernel trick.\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "731d7212-e568-4d93-b10f-73886a4e8ca6",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"Q2. How Can We Implement an SVM with a Polynomial Kernel in Python Using Scikit-learn?\"\n",
    "from sklearn import datasets\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Load the Iris dataset\n",
    "iris = datasets.load_iris()\n",
    "X = iris.data\n",
    "y = iris.target\n",
    "\n",
    "# Split the dataset into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "\n",
    "# Create an SVM classifier with a polynomial kernel\n",
    "svm_model = SVC(kernel='poly', degree=3, C=1.0)  # degree=3 specifies a cubic polynomial kernel\n",
    "svm_model.fit(X_train, y_train)\n",
    "\n",
    "# Predict the labels for the test set\n",
    "y_pred = svm_model.predict(X_test)\n",
    "\n",
    "# Evaluate the performance using accuracy\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(f'Accuracy: {accuracy}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "587dcb77-5cde-43b3-bcb5-8183a5040d64",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Q3. How Does Increasing the Value of Epsilon Affect the Number of Support Vectors in SVR?\n",
    "In Support Vector Regression (SVR), the epsilon (\n",
    "𝜖\n",
    "ϵ) parameter defines a margin of tolerance where no penalty is given to errors (i.e., the distance between the predicted and actual values) that fall within this margin.\n",
    "\n",
    "Increasing \n",
    "𝜖\n",
    "ϵ:\n",
    "Allows more data points to fall within the margin without being penalized.\n",
    "As a result, fewer support vectors are needed because more data points are considered \"within tolerance\" and do not influence the model.\n",
    "Decreasing \n",
    "𝜖\n",
    "ϵ:\n",
    "Reduces the margin, so more data points will lie outside this margin and become support vectors.\n",
    "In summary, increasing \n",
    "𝜖\n",
    "ϵ typically decreases the number of support vectors, while decreasing \n",
    "𝜖\n",
    "ϵ increases the number of support vectors.\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "450d3646-08bf-4944-86a0-a0629cd64205",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Q4. How Does the Choice of Kernel Function, C Parameter, Epsilon Parameter, and Gamma Parameter Affect the Performance of Support Vector Regression (SVR)?\n",
    "1. Kernel Function:\n",
    "\n",
    "The kernel function determines the shape of the decision boundary. Common choices are the linear, polynomial, and RBF (Radial Basis Function) kernels.\n",
    "When to use:\n",
    "Linear Kernel: When the data is linearly separable.\n",
    "Polynomial Kernel: When the data has polynomial relationships.\n",
    "RBF Kernel: When the data is not linearly separable and exhibits complex patterns.\n",
    "2. C Parameter (Regularization Parameter):\n",
    "\n",
    "Controls the trade-off between a smooth decision boundary and classifying the training points correctly.\n",
    "Small C: The model is more regularized, allowing some misclassification but yielding a smoother decision boundary.\n",
    "Large C: The model aims to classify all training points correctly, which may lead to overfitting.\n",
    "When to use:\n",
    "Small C: Use when you want to avoid overfitting and prefer a smoother model.\n",
    "Large C: Use when you want to minimize training error.\n",
    "3. Epsilon Parameter (\n",
    "𝜖\n",
    "ϵ):\n",
    "\n",
    "In SVR, \n",
    "𝜖\n",
    "ϵ defines a margin of tolerance where errors are not penalized.\n",
    "Small \n",
    "𝜖\n",
    "ϵ: The model will try to predict as close to the actual values as possible, leading to more support vectors.\n",
    "Large \n",
    "𝜖\n",
    "ϵ: The model will allow more error without penalizing, leading to fewer support vectors.\n",
    "When to use:\n",
    "Small \n",
    "𝜖\n",
    "ϵ: Use when high accuracy is needed, and you can tolerate more complexity.\n",
    "Large \n",
    "𝜖\n",
    "ϵ: Use when you can tolerate some error and want a simpler model.\n",
    "4. Gamma Parameter (for RBF Kernel):\n",
    "\n",
    "Controls the influence of a single training example.\n",
    "Small Gamma: A point far away from the margin can still influence the decision boundary, leading to a smoother model.\n",
    "Large Gamma: The decision boundary is influenced only by points very close to it, leading to a more complex model that may overfit.\n",
    "When to use:\n",
    "Small Gamma: Use when you expect the data to be less complex or when you want to avoid overfitting.\n",
    "Large Gamma: Use when you expect the data to be complex and want to capture intricate patterns.\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b33002b8-9768-4253-b85d-2a48d0f0fe2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Q5. Assignment\n",
    "Let's walk through the assignment step by step:\n",
    "\n",
    "1. Import the Necessary Libraries and Load the Dataset:\n",
    "'''\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "import joblib\n",
    "\n",
    "# Load dataset (Example: Iris dataset)\n",
    "from sklearn import datasets\n",
    "iris = datasets.load_iris()\n",
    "X = iris.data\n",
    "y = iris.target\n",
    "\n",
    "\"2. Split the Dataset into Training and Testing Sets:\"\n",
    "# Split the dataset\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "\n",
    "\"3. Preprocess the Data (e.g., Scaling):\"\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
