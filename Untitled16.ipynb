{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7c0aea2-e981-4372-a3ac-2be8a33f2c3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Q1: Explain the concept of R-squared in linear regression models. How is it calculated, and what does it represent?\n",
    "\n",
    "\n",
    "\n",
    "R-squared (\n",
    "𝑅\n",
    "2\n",
    "R \n",
    "2\n",
    " ) is a statistical measure that represents the proportion of the variance in the dependent variable that is predictable from the \n",
    " independent variables. It is a key indicator of how well the regression model fits the data.\n",
    "\n",
    "Calculation:\n",
    "𝑅\n",
    "2\n",
    "=\n",
    "1\n",
    "−\n",
    "SS\n",
    "res\n",
    "SS\n",
    "tot\n",
    "R \n",
    "2\n",
    " =1− \n",
    "SS \n",
    "tot\n",
    "​\n",
    " \n",
    "SS \n",
    "res\n",
    "​\n",
    " \n",
    "​\n",
    " \n",
    "where:\n",
    "\n",
    "SS\n",
    "res\n",
    "SS \n",
    "res\n",
    "​\n",
    "  is the sum of squares of the residuals (the difference between observed and predicted values).\n",
    "SS\n",
    "tot\n",
    "SS \n",
    "tot\n",
    "​\n",
    "  is the total sum of squares (the difference between observed values and the mean of observed values).\n",
    "What It Represents:\n",
    "\n",
    "𝑅\n",
    "2\n",
    "=\n",
    "0\n",
    "R \n",
    "2\n",
    " =0 indicates that the model does not explain any of the variability of the response data around its mean.\n",
    "𝑅\n",
    "2\n",
    "=\n",
    "1\n",
    "R \n",
    "2\n",
    " =1 indicates that the model explains all the variability of the response data around its mean.\n",
    "Higher \n",
    "𝑅\n",
    "2\n",
    "R \n",
    "2\n",
    "  values indicate a better fit of the model to the data.\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d661213d-8e93-46ec-8cf8-2df7546a2fcd",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Q2: Define adjusted R-squared and explain how it differs from the regular R-squared.\n",
    "\n",
    "\n",
    "Adjusted R-squared is a modified version of \n",
    "𝑅\n",
    "2\n",
    "R \n",
    "2\n",
    "  that adjusts for the number of independent variables in the model. Unlike \n",
    "𝑅\n",
    "2\n",
    "R \n",
    "2\n",
    " , which always increases as more variables are added to the model, adjusted \n",
    "𝑅\n",
    "2\n",
    "R \n",
    "2\n",
    "  accounts for the model complexity by penalizing the addition of irrelevant variables.\n",
    "\n",
    "Calculation:\n",
    "Adjusted \n",
    "𝑅\n",
    "2\n",
    "=\n",
    "1\n",
    "−\n",
    "(\n",
    "(\n",
    "1\n",
    "−\n",
    "𝑅\n",
    "2\n",
    ")\n",
    "(\n",
    "𝑛\n",
    "−\n",
    "1\n",
    ")\n",
    "𝑛\n",
    "−\n",
    "𝑘\n",
    "−\n",
    "1\n",
    ")\n",
    "Adjusted R \n",
    "2\n",
    " =1−( \n",
    "n−k−1\n",
    "(1−R \n",
    "2\n",
    " )(n−1)\n",
    "​\n",
    " )\n",
    "where:\n",
    "\n",
    "𝑛\n",
    "n is the number of observations.\n",
    "𝑘\n",
    "k is the number of independent variables.\n",
    "Difference:\n",
    "\n",
    "Regular \n",
    "𝑅\n",
    "2\n",
    "R \n",
    "2\n",
    "  can be misleading when comparing models with different numbers of predictors because it does not account for the number of variables.\n",
    "Adjusted \n",
    "𝑅\n",
    "2\n",
    "R \n",
    "2\n",
    "  provides a more accurate comparison by penalizing models that add variables without improving the model's fit significantly.\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bfbd89f-84a6-48d4-bcd5-412282b7d58b",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Q3: When is it more appropriate to use adjusted R-squared?\n",
    "\n",
    "\n",
    "Adjusted R-squared is more appropriate to use when comparing models with a different number of predictors. It helps to determine whether the inclusion \n",
    "of additional variables is truly improving the model’s performance or if it’s merely increasing the complexity without providing a better fit.\n",
    "\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f06518e-ae14-420a-b937-af7315a4f504",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Q4: What are RMSE, MSE, and MAE in the context of regression analysis? How are these metrics calculated, and what do they represent?\n",
    "RMSE (Root Mean Square Error), MSE (Mean Squared Error), and MAE (Mean Absolute Error) are evaluation metrics used to measure the accuracy of a \n",
    "regression model by quantifying the difference between the observed and predicted values.\n",
    "\n",
    "Mean Squared Error (MSE):\n",
    "MSE\n",
    "=\n",
    "1\n",
    "𝑛\n",
    "∑\n",
    "𝑖\n",
    "=\n",
    "1\n",
    "𝑛\n",
    "(\n",
    "𝑦\n",
    "𝑖\n",
    "−\n",
    "𝑦\n",
    "^\n",
    "𝑖\n",
    ")\n",
    "2\n",
    "MSE= \n",
    "n\n",
    "1\n",
    "​\n",
    " ∑ \n",
    "i=1\n",
    "n\n",
    "​\n",
    " (y \n",
    "i\n",
    "​\n",
    " − \n",
    "y\n",
    "^\n",
    "​\n",
    "  \n",
    "i\n",
    "​\n",
    " ) \n",
    "2\n",
    " \n",
    "\n",
    "Calculation: It is the average of the squared differences between observed (\n",
    "𝑦\n",
    "𝑖\n",
    "y \n",
    "i\n",
    "​\n",
    " ) and predicted (\n",
    "𝑦\n",
    "^\n",
    "𝑖\n",
    "y\n",
    "^\n",
    "​\n",
    "  \n",
    "i\n",
    "​\n",
    " ) values.\n",
    "Representation: MSE gives a higher penalty to larger errors due to the squaring of differences.\n",
    "Root Mean Square Error (RMSE):\n",
    "RMSE\n",
    "=\n",
    "MSE\n",
    "RMSE= \n",
    "MSE\n",
    "​\n",
    " \n",
    "\n",
    "Calculation: It is the square root of the MSE.\n",
    "Representation: RMSE is in the same units as the dependent variable, making it easier to interpret compared to MSE.\n",
    "Mean Absolute Error (MAE):\n",
    "MAE\n",
    "=\n",
    "1\n",
    "𝑛\n",
    "∑\n",
    "𝑖\n",
    "=\n",
    "1\n",
    "𝑛\n",
    "∣\n",
    "𝑦\n",
    "𝑖\n",
    "−\n",
    "𝑦\n",
    "^\n",
    "𝑖\n",
    "∣\n",
    "MAE= \n",
    "n\n",
    "1\n",
    "​\n",
    " ∑ \n",
    "i=1\n",
    "n\n",
    "​\n",
    " ∣y \n",
    "i\n",
    "​\n",
    " − \n",
    "y\n",
    "^\n",
    "​\n",
    "  \n",
    "i\n",
    "​\n",
    " ∣\n",
    "\n",
    "Calculation: It is the average of the absolute differences between observed and predicted values.\n",
    "Representation: MAE provides a linear score without penalizing larger errors more heavily than smaller ones.\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cf0f357-d98f-4feb-88b5-21441e30448c",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''Q5: Discuss the advantages and disadvantages of using RMSE, MSE, and MAE as evaluation metrics in regression analysis.\n",
    "\n",
    "Advantages:\n",
    "\n",
    "MSE: Sensitive to outliers, which can be useful if larger errors are particularly undesirable in a given context.\n",
    "RMSE: Also sensitive to outliers and provides a metric in the same units as the dependent variable, making it more interpretable.\n",
    "MAE: Less sensitive to outliers, offering a more robust measure of central tendency that is easier to understand and interpret.\n",
    "Disadvantages:\n",
    "\n",
    "MSE: The units of MSE are squared, making it less interpretable compared to other metrics.\n",
    "RMSE: Can be overly influenced by large errors due to its reliance on squaring differences, potentially leading to skewed interpretations.\n",
    "MAE: Does not differentiate between the magnitude of different errors, which might be a limitation if larger errors need to be penalized more.'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ab191e5-9b12-4e38-bf9e-ca464de9dcaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Q6: Explain the concept of Lasso regularization. How does it differ from Ridge regularization, and when is it more appropriate to use?\n",
    "Lasso (Least Absolute Shrinkage and Selection Operator) Regularization is a technique that adds a penalty to the regression model based on the \n",
    "absolute value of the coefficients, encouraging simpler models by shrinking some coefficients to exactly zero.\n",
    "\n",
    "Lasso Regression Formula:\n",
    "Minimize \n",
    "∑\n",
    "𝑖\n",
    "=\n",
    "1\n",
    "𝑛\n",
    "(\n",
    "𝑦\n",
    "𝑖\n",
    "−\n",
    "∑\n",
    "𝑗\n",
    "=\n",
    "1\n",
    "𝑝\n",
    "𝛽\n",
    "𝑗\n",
    "𝑋\n",
    "𝑖\n",
    "𝑗\n",
    ")\n",
    "2\n",
    "+\n",
    "𝜆\n",
    "∑\n",
    "𝑗\n",
    "=\n",
    "1\n",
    "𝑝\n",
    "∣\n",
    "𝛽\n",
    "𝑗\n",
    "∣\n",
    "Minimize ∑ \n",
    "i=1\n",
    "n\n",
    "​\n",
    " (y \n",
    "i\n",
    "​\n",
    " −∑ \n",
    "j=1\n",
    "p\n",
    "​\n",
    " β \n",
    "j\n",
    "​\n",
    " X \n",
    "ij\n",
    "​\n",
    " ) \n",
    "2\n",
    " +λ∑ \n",
    "j=1\n",
    "p\n",
    "​\n",
    " ∣β \n",
    "j\n",
    "​\n",
    " ∣\n",
    "where:\n",
    "\n",
    "𝜆\n",
    "λ is the regularization parameter that controls the strength of the penalty.\n",
    "𝛽\n",
    "𝑗\n",
    "β \n",
    "j\n",
    "​\n",
    "  are the coefficients of the independent variables.\n",
    "Difference from Ridge Regularization:\n",
    "\n",
    "Ridge Regression: Penalizes the sum of squared coefficients (\n",
    "∑\n",
    "𝛽\n",
    "𝑗\n",
    "2\n",
    "∑β \n",
    "j\n",
    "2\n",
    "​\n",
    " ), leading to small but non-zero coefficients.\n",
    "Lasso Regression: Penalizes the sum of absolute coefficients (\n",
    "∑\n",
    "∣\n",
    "𝛽\n",
    "𝑗\n",
    "∣\n",
    "∑∣β \n",
    "j\n",
    "​\n",
    " ∣), leading to some coefficients being exactly zero, effectively performing feature selection.\n",
    "When to Use:\n",
    "\n",
    "Lasso is more appropriate when you expect some features to be irrelevant and want to perform feature selection.\n",
    "Ridge is better when all features are believed to be relevant but should be regularized to prevent overfitting.\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b61b2eb-c5f7-4401-ba70-efb0fc73eaa8",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Q7: How do regularized linear models help to prevent overfitting in machine learning? Provide an example to illustrate.\n",
    "Regularized Linear Models introduce a penalty for larger coefficients in the regression model, discouraging the model from fitting the training \n",
    "data too closely. This penalization reduces the \n",
    "model’s variance, helping it generalize better to unseen data.\n",
    "\n",
    "Example:\n",
    "\n",
    "Suppose we have a dataset with a large number of features. A standard linear regression model may overfit, leading to poor performance on new data.\n",
    "By applying Ridge or Lasso regularization, the model is forced to keep the coefficients small or even eliminate some entirely (in the case of Lasso),\n",
    "resulting in a simpler, more generalizable model.\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "937d3c72-602f-402d-aee2-8ef217d6ec45",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Q8: Discuss the limitations of regularized linear models and explain why they may not always be the best choice for regression analysis.\n",
    "Limitations:\n",
    "\n",
    "Bias-Variance Tradeoff: Regularization introduces bias into the model, which may result in underfitting if the regularization is too strong.\n",
    "Interpretability: Regularization can make the interpretation of coefficients more difficult, particularly when Lasso sets some coefficients to zero.\n",
    "Not Suitable for All Models: Regularization is not always necessary or beneficial, particularly when the number of features is small or when the \n",
    "features are carefully selected.\n",
    "Why They May Not Always Be the Best Choice:\n",
    "\n",
    "In situations where the model already has a low number of features and is not prone to overfitting, regularization might unnecessarily complicate\n",
    "the model.\n",
    "For models where interpretability is crucial, the shrinkage of coefficients due to regularization may obscure the understanding of feature importance.\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1660a10-ba59-4181-8621-a1905be5b72c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
